{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb82c0f8",
   "metadata": {},
   "source": [
    "# Data cleaning and exploration (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T19:28:45.650283Z",
     "start_time": "2023-10-30T19:28:45.587655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size (7613, 5)\n",
      "Testing set size (3263, 4)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('./data/train.csv', dtype={'id': np.int16, 'target': np.int8})\n",
    "df_test = pd.read_csv('./data/test.csv', dtype={'id': np.int16})\n",
    "\n",
    "print(f\"Training set size {df_train.shape}\")\n",
    "print(f\"Testing set size {df_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281b5438",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T20:00:55.605166Z",
     "start_time": "2023-10-30T20:00:55.564903Z"
    }
   },
   "source": [
    "## 1.1 missing value and duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "43952d05818bd752",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T19:33:23.133478Z",
     "start_time": "2023-10-30T19:33:23.125890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value ration in Training set:\n",
      "keyword : 0.80%\n",
      "location : 33.27%\n",
      "text : 0.00%\n",
      "Missing value in Testing set:\n",
      "keyword : 0.80%\n",
      "location : 33.86%\n",
      "text : 0.00%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>no_location</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>no_location</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>no_location</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>no_location</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>no_location</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>no_location</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>no_location</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>no_location</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>no_location</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>no_location</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id     keyword     location  \\\n",
       "0         1  no_keyword  no_location   \n",
       "1         4  no_keyword  no_location   \n",
       "2         5  no_keyword  no_location   \n",
       "3         6  no_keyword  no_location   \n",
       "4         7  no_keyword  no_location   \n",
       "...     ...         ...          ...   \n",
       "7608  10869  no_keyword  no_location   \n",
       "7609  10870  no_keyword  no_location   \n",
       "7610  10871  no_keyword  no_location   \n",
       "7611  10872  no_keyword  no_location   \n",
       "7612  10873  no_keyword  no_location   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['keyword', 'location', 'text']\n",
    "\n",
    "print(f\"Missing value ration in Training set:\")\n",
    "for feature in features:\n",
    "    print(f\"{feature} : {df_train[feature].isnull().sum() / df_train.shape[0] * 100 :.2f}%\")\n",
    "    df_train[feature] = df_train[feature].fillna(f\"no_{feature}\")\n",
    "\n",
    "\n",
    "print(f\"Missing value in Testing set:\")\n",
    "for feature in features:\n",
    "    print(f\"{feature} : {df_test[feature].isnull().sum() / df_test.shape[0]* 100 :.2f}%\")\n",
    "    df_test[feature] = df_test[feature].fillna(f\"no_{feature}\")\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fb9f51059fb0123a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T20:01:00.335101Z",
     "start_time": "2023-10-30T20:01:00.326187Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7485, 5)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete all rows with same text but different target, keep one duplicate of same text and targte\n",
    "df_train = df_train.drop_duplicates(subset=['text', 'target'], keep='first')\n",
    "contrast_target = df_train[df_train.duplicated(subset=['text'], keep=False)]\n",
    "df_train = df_train.drop(contrast_target.index)\n",
    "df_train = df_train.sort_index()\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ba30aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T20:01:02.422868Z",
     "start_time": "2023-10-30T20:01:02.399277Z"
    }
   },
   "source": [
    "## 1.2 Text cleaning\n",
    "\n",
    "Urls are removed\n",
    "\n",
    "Special characters (non ASCII)\n",
    "\n",
    "Contractions are expanded\n",
    "\n",
    "Punctuations #, @, !, ?, +, &, -, $, =, <, >, |, {, }, ^, ', (, ),[, ], *, \\%, ..., ', ., :, ; \n",
    "\n",
    "Character entity references are replaced with their actual symbols\n",
    "\n",
    "\n",
    "Finally, hashtags and usernames contain lots of information about the context but they are written without spaces in between words so they don't have embeddings. Informational usernames and hashtags should be expanded but there are too many of them. I expanded as many as I could, but it takes too much time to run clean function after adding those replace calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3258e576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "contractions = { \n",
    "\"aren't\": \"are not\",\n",
    "\"aren't\": \"are not\",    \n",
    "\"can't\": \"cannot\",\n",
    "\"cant\": \"cannot\",    \n",
    "\"can't've\": \"cannot have\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldnt\": \"could not\",    \n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"didnt\": \"did not\",    \n",
    "\"doesn't\": \"does not\",\n",
    "\"doesnt\": \"does not\",    \n",
    "\"don't\": \"do not\",\n",
    "\"dont\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadnt\": \"had not\",    \n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"hasnt\": \"has not\",    \n",
    "\"haven't\": \"have not\",\n",
    "\"havent\": \"have not\",    \n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"hes\": \"he is\",    \n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"isnt\": \"is not\",    \n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"lets\": \"let us\",    \n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"shes\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"thats\": \"that is\",    \n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"theres\": \"there is\",    \n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"theyll\": \"they will\",    \n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"theyre\": \"they are\",    \n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"wasnt\": \"was not\",    \n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"whats\" : \"what is\",    \n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"wheres\": \"where is\",    \n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"wont\": \"will not\",    \n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldnt\": \"would not\",    \n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"yall\": \"you all\",    \n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"youd\": \"you would\",    \n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"youre\": \"you are\",    \n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5e4c175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # lower\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Urls and @username\n",
    "    text = re.sub(r\"https?://t.co/\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    \n",
    "    # spaces, tabs\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    # Contractions\n",
    "    text = re.sub(r\"\\x89Ã›Âª\", \"\\'\", text)\n",
    "    for word, expanded in contractions.items():\n",
    "        text = re.sub(word, expanded, text)\n",
    "        \n",
    "    # non ASCII characters\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)\n",
    "    \n",
    "    # Character entity references\n",
    "    text = re.sub(r\"&gt;\", \">\", text)\n",
    "    text = re.sub(r\"&lt;\", \"<\", text)\n",
    "    text = re.sub(r\"&amp;\", \"&\", text)\n",
    "    \n",
    "    # Words with punctuations\n",
    "    \n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n",
    "    for p in punctuations:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2a9cf9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text_cleaned'] = df_train['text'].apply(lambda s : clean_text(s))\n",
    "df_train.to_csv('./data/train_cleaned.csv')\n",
    "df_test['text_cleaned'] = df_test['text'].apply(lambda s : clean_text(s))\n",
    "df_test.to_csv('./data/test_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8a6c9274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "62ee29b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2073, 2003, 26779, 1999, 1996, 2088, 1012, 1012, 1012, 4949, 1029, 1030, 2567, 1045, 1005, 1049, 2025, 2017, 1005, 2128, 2025, 4995, 1005, 1056, 102]\n",
      "['[CLS]', 'where', 'is', 'himalayas', 'in', 'the', 'world', '.', '.', '.', 'map', '?', '@', 'brother', 'i', \"'\", 'm', 'not', 'you', \"'\", 're', 'not', 'aren', \"'\", 't', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "sample = 'where is Himalayas in the world... map? @brother I\\'m  not you\\'re not aren\\'t'\n",
    "encoding = tokenizer.encode(sample)\n",
    "print(encoding)\n",
    "print(tokenizer.convert_ids_to_tokens(encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a0e555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
